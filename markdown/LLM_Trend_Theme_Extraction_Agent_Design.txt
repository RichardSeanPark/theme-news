언급량 급증 테마 추출 AI 에이전트 설계
1. AI 에이전트 아키텍처 및 구조 설계
LLM 중심 파이프라인: 전체 시스템은 LLM을 중심으로 동작하는 에이전트 형태로 구성한다. 콘솔 기반의 Python 스크립트 등으로 구현하며, 단계별로 LLM을 호출하여 작업을 수행한다. 각 단계는 모듈화하여 향후 블로그/SNS 자동 발행 단계가 추가되더라도 구조를 확장하기 쉽도록 설계한다. 주요 구성 요소와 LLM 역할은 다음과 같다:
데이터 수집 모듈: 뉴스, 블로그, 카페 등에서 최근 24시간 내 텍스트 데이터를 수집한다 (2~3절 참고). 수집된 데이터는 에이전트의 입력으로 제공된다. 입력 데이터 포맷은 JSON 또는 Python 객체 형태로 구성한다. 예를 들어 뉴스 기사 리스트와 블로그 글 리스트를 포함하는 JSON:

{
  "news": [
    {"title": "기사 제목1", "content": "기사 본문 내용...", "source": "연합뉴스", "published": "2025-04-20T09:00:00+09:00"},
    {"title": "기사 제목2", "content": "기사 본문 내용...", "source": "NYTimes", "published": "2025-04-20T08:30:00Z"},
    ... 
  ],
  "blogs": [
    {"title": "블로그 글 제목1", "content": "블로그 글 본문...", "author": "블로거명", "published": "2025-04-20T08:10:00+09:00"},
    {"title": "블로그 글 제목2", "content": "블로그 글 본문...", "author": "블로거명", "published": "2025-04-20T07:50:00+09:00"},
    ...
  ],
  "cafes": [
    {"title": "카페 글 제목1", "content": "카페 글 본문...", "cafe": "카페이름", "published": "2025-04-20T08:05:00+09:00"},
    ...
  ]
}


각 항목에는 출처 구분과 작성 시각을 포함하여, 이후 언급량 증가(증감) 분석과 출처별 통계에 활용한다.
LLM 에이전트 (1단계: 키워드 추출): 에이전트는 수집된 텍스트 데이터를 LLM에 전달하여 주요 키워드/엔티티 후보를 추출한다. 많은 양의 본문을 직접 입력하기 어렵다면, 전처리로 각 문서의 핵심만 추려 LLM 프롬프트에 포함한다. 예를 들어 각 뉴스의 제목과 첫 문장, 블로그 글의 제목과 요약 등을 합쳐 LLM 입력으로 사용한다. LLM에게는 이 텍스트에서 빈번히 언급되는 인물, 조직, 이벤트, 핵심 명사구 등을 식별하도록 지시한다. 이 단계 출력은 키워드 후보 목록 (예: ["전기차 배터리", "프로야구 개막전", "금리 인상", ...])이며, 필요하면 각 키워드의 빈도수도 함께 집계한다. (빈도 집계는 LLM이 아닌 파이프라인 코드에서 수행 가능)
LLM 에이전트 (2단계: 의미 군집화): 추출된 키워드 후보들을 LLM에 다시 입력하여 의미적으로 유사한 것들을 군집화한다. LLM은 언어적 유사성을 기반으로 동의어나 동일한 주제를 가리키는 키워드들을 그룹화하고, 각 그룹에 대표 테마 이름을 지정한다. 예를 들어 ["전기차", "배터리", "EV"] 키워드들을 「전기차 배터리 기술」 테마로 묶는 식이다. 이 단계에서는 LLM의 풍부한 언어 이해력을 활용해 수작업 없이도 주제별 클러스터링을 수행한다​
blog.hyperarc.com
. 출력은 예컨대 다음과 같은 JSON 구조가 된다:


[
  {"theme": "전기차 배터리 기술", "keywords": ["전기차", "배터리", "EV"], "total_mentions": 32},
  {"theme": "프로야구 개막전", "keywords": ["프로야구", "개막전", "KBO"], "total_mentions": 25},
  ...
]


여기서 total_mentions는 해당 키워드들의 총 언급량이며, 이는 사전에 집계한 빈도 데이터를 활용해 계산한다 (LLM에게 계산시키기보다는 코드에서 합산).
에이전트 (3단계: 급상승 판단): 테마별 언급량이 평균 대비 얼마나 증가했는지를 판단하여 급상승 트렌드인지 결정한다. 이 부분은 통계 계산이므로 LLM보다는 외부 로직으로 수행한다. 과거 기간(예: 이전날 또는 지난주 동일시간대) 대비 현재 언급량의 **z-스코어(z-score)**를 계산한다​
stackoverflow.com
. z-스코어는 (현재 언급량 - 과거 평균) / 과거 표준편차로 정의되며, 값이 높을수록 이례적으로 언급량이 증가했음을 의미한다​
stackoverflow.com
. 모든 테마에 대해 z-스코어를 구한 뒤, 상위 20개 정도의 높은 z-스코어 테마를 선정한다. (예: z-스코어 상위 20개 = 급상승 테마 Top 20)
LLM 에이전트 (4단계: 테마 요약 및 출력): 최종 선정된 상위 테마들에 대해 LLM이 각 테마별 간단 요약을 작성한다. 각 테마에 속한 키워드와 출처별 언급 정보를 LLM에 제공하고, “이 테마에서 어떤 일이 있었는지 한두 문장으로 요약하라”는 프롬프트를 준다. 예를 들어 {"theme": "전기차 배터리 기술", "keywords": ["전기차", "배터리"], "mentions": {"news":20,"blog":8,"cafe":4}}를 입력으로 LLM이 “완성차 업체들의 신형 전기차 배터리 기술 발표와 관련 투자 소식이 쏟아졌다”와 같은 요약 문장을 생성하게 한다.
이상의 모듈을 거쳐 에이전트는 최종 결과로 상위 20개 급상승 테마 목록을 생성한다. 콘솔에서는 이 목록을 사람이 읽기 좋은 형식으로 출력하며, 동시에 JSON 등 구조화된 형태로도 제공해 향후 블로그/SNS 자동 발행 모듈에 데이터를 넘겨줄 수 있도록 한다. 전체 아키텍처를 요약하면 다음과 같다:
데이터 수집 → 키워드 추출(LLM) → 의미 군집화 및 테마화(LLM) → 통계적 트렌드 분석(z-score) → 상위 테마 요약(LLM) → 출력 및 저장.
이때 LLM 호출은 API (예: OpenAI GPT-4 API)나 오픈소스 LLM 모델(Gemini/Gemma 27B 등)을 로컬서버에서 추론하는 방식으로 수행한다. 각각의 프롬프트 설계와 입력/출력 포맷은 다음 절들에서 구체적으로 제시한다.
2. 뉴스 데이터 수집
공개 뉴스 API 활용: 최신 뉴스를 수집하기 위해 뉴스 제공 API를 사용한다. 한국과 미국을 중심으로 다음과 같은 오픈 API를 고려할 수 있다:
NewsAPI (newsapi.org): 전 세계 뉴스 기사를 통합 제공하는 유명한 REST API로, 한국과 미국 언론사를 포함한 다수의 뉴스 소스를 지원한다​
mayurashinde.medium.com
. 사용 방법: NewsAPI에 개발자 등록을 하여 API 키를 발급받은 뒤, HTTP GET 요청을 통해 뉴스를 검색한다. 예를 들어 미국 최신 헤드라인은 다음과 같이 요청할 수 있다:

curl "https://newsapi.org/v2/top-headlines?country=us&apiKey=<YOUR_API_KEY>"


한국 뉴스 헤드라인은 country 파라미터를 kr로 지정한다​
newsapi.org
:

curl "https://newsapi.org/v2/top-headlines?country=kr&category=general&apiKey=<YOUR_API_KEY>"


위 요청은 국가별 주요 뉴스를 JSON으로 반환하며, articles 배열 내에 여러 기사 객체가 포함된다. 각 기사 객체는 일반적으로 title(제목), description(요약), url(원문 링크), publishedAt(발행시각), source(매체명) 등을 속성으로 가진다​
mayurashinde.medium.com
. 응답 예시:

{
  "status": "ok",
  "totalResults": 34,
  "articles": [
    {
      "source": {"id": "cnn", "name": "CNN"},
      "author": "....",
      "title": "바이든 대통령, ...", 
      "description": "미국 대통령이 ...",
      "url": "http://cnn.com/....",
      "publishedAt": "2025-04-20T02:30:00Z",
      "content": "기사 본문 일부 내용 ..."
    },
    ...
  ]
}


content 필드는 기사의 본문 일부를 제공하지만, 보통 몇백 자로 잘려 있을 수 있다. 본문 전체를 확보하려면 url을 통해 해당 뉴스 웹사이트를 크롤링/파싱해야 한다. NewsAPI의 /v2/everything 엔드포인트를 사용하면 키워드나 날짜 범위로 기사를 검색할 수도 있다​
newsapi.org
. 예를 들어 지난 24시간 내 “Apple” 관련 기사를 검색:

curl "https://newsapi.org/v2/everything?q=Apple&from=2025-04-19&to=2025-04-20&sortBy=publishedAt&apiKey=<API_KEY>"


이렇게 하면 특정 키워드의 하루치 기사를 얻을 수 있고, from/to 파라미터로 날짜 필터링을 지정한다. NewsAPI는 무료 플랜의 경우 분당/일일 호출 제한이 있으므로 주의한다.
뉴욕타임스 API (NYTimes API): 미국 주요 매체별로 제공되는 공개 API의 예로, 뉴욕타임스는 자체 Developer API를 제공한다. Top Stories API를 사용하면 각 섹션별 최신 기사를 가져올 수 있다. 예:

GET https://api.nytimes.com/svc/topstories/v2/world.json?api-key=<NYT_API_KEY>


마찬가지로 JSON으로 기사 목록을 반환하며, results 배열에 각 기사의 title, abstract(요약), url, published_date 등이 포함된다. 뉴욕타임스 Article Search API를 이용하면 쿼리와 날짜로 기사를 검색 가능하다 (예: begin_date=20250419 와 end_date=20250420로 24시간 내 검색).
네이버 뉴스 검색 API: 네이버 검색 서비스를 통해 한국 뉴스 기사를 가져오는 REST API이다. 사용 방법: 네이버 개발자 센터에서 애플리케이션 등록 후 발급받은 Client ID와 Secret을 HTTP 헤더에 포함하여 요청한다​
developers.naver.com
. GET 요청 URL은 https://openapi.naver.com/v1/search/news.json이며, query 파라미터로 검색어를 지정해야 한다​
developers.naver.com
. 예를 들어 “주식” 키워드의 최신 뉴스 10건 검색:

curl "https://openapi.naver.com/v1/search/news.json?query=%EC%A3%BC%EC%8B%9D&display=10&sort=date" \
     -H "X-Naver-Client-Id: <클라이언트ID>" \
     -H "X-Naver-Client-Secret: <시크릿>"


여기서 display는 한 번에 출력할 결과 수(기본 10, 최대 100)이며​
developers.naver.com
, sort=date로 날짜순 정렬을 지정했다​
developers.naver.com
. 응답 JSON에는 items 배열로 뉴스 결과 목록이 온다. 각 item은 title(제목), originallink(언론사 원문 URL), link(네이버 뉴스 URL 또는 원문 URL), description(요약), pubDate(발행일시) 등을 포함한다. 예시:

{
  "lastBuildDate": "Sun, 20 Apr 2025 15:00:00 +0900",
  "total": 12345,
  "start": 1,
  "display": 10,
  "items": [
    {
      "title": "증시 상승세...<b>주식</b> 투자 관심↑",
      "originallink": "http://news.example.com/...", 
      "link": "http://news.naver.com/...",
      "description": "...<b>주식</b>에 대한 기사 요약...",
      "pubDate": "Sun, 20 Apr 2025 14:55:00 +0900"
    },
    ...
  ]
}


주의: 네이버 뉴스검색 API는 query가 필수이므로, 특정 키워드 없이 전체 뉴스를 가져오기는 어렵다. 따라서 보통 트렌드가 의심되는 키워드를 미리 정해 검색하거나, 혹은 "오늘" 같은 일반 단어를 넣어 최대한 폭넓게 기사를 수집하는 방식이 있다. NewsAPI 같은 통합 API와併用하여 한국 뉴스를 보강하는 용도로 쓰는 것이 일반적이다.
수집한 뉴스 데이터는 JSON 형태로 에이전트에 전달되며, 앞서 설명한 바와 같이 content 본문이 없을 경우 url을 통해 추가 크롤링이 필요하다. Python의 requests와 BeautifulSoup 혹은 Newspaper3k 라이브러리 등을 활용하면 뉴스 원문 웹페이지에서 본문 텍스트를 추출할 수 있다. 크롤링 시 해당 사이트의 robots.txt 정책을 준수하고, 과도한 요청을 보내지 않도록 주의한다.
3. 블로그/카페 데이터 수집
뉴스 외에 블로그 및 카페 게시글 데이터를 수집하여 사용자 생성 콘텐츠에서의 언급 동향도 파악한다. 주로 네이버 블로그와 네이버 카페를 대상으로 공개 API를 활용하거나, 필요한 경우 웹 크롤링을 병행한다.
네이버 블로그 검색 API: 네이버의 블로그 글 검색 결과를 제공하는 REST API이다. 사용법은 앞서 뉴스 API와 유사하게 Client ID/Secret을 헤더에 보내며, 요청 URL은 https://openapi.naver.com/v1/search/blog.json이다​
developers.naver.com
. query 파라미터로 검색어를 지정하고, display (최대 100)와 sort 옵션을 사용할 수 있다​
developers.naver.com
. 예를 들어 특정 키워드가 없는 최신 블로그 글을 확보하기 위해, 모든 글에 공통으로 들어갈 만한 일반 단어를 검색어로 활용할 수 있다. "오늘" 같은 단어는 많은 글에서 등장하므로, 이를 넣고 날짜순 정렬하면 사실상 가장 최근에 작성된 블로그 글 목록을 얻는 효과가 있다. 예시 요청:

curl "https://openapi.naver.com/v1/search/blog.json?query=%EC%98%A4%EB%8A%98&display=100&sort=date" \
     -H "X-Naver-Client-Id: <클라이언트ID>" \
     -H "X-Naver-Client-Secret: <시크릿>"


위 요청은 **“오늘”**을 포함한 최신 블로그 게시글 최대 100건을 JSON으로 반환한다. 응답 JSON 구조는 items 배열에 각 블로그 글의 정보를 담는다​
developers.naver.com
:

{
  "title": "블로그 글 제목...<b>오늘</b>...",
  "link": "https://blog.naver.com/....", 
  "description": "본문 요약...<b>오늘</b>...",
  "bloggername": "블로거닉네임",
  "bloggerlink": "https://blog.naver.com/블로그ID",
  "postdate": "20250420"
}


여기서 postdate는 YYYYMMDD 형식의 작성일​
developers.naver.com
이며, title과 description에는 검색어 부분이 <b> 태그로 강조되어 온다. 주의: 검색어로 사용한 일반 단어가 해당 글에 없어도 결과에서 빠질 수 있으므로, 더 포괄적인 쿼리를 쓰거나 여러 쿼리를 조합해 샘플링 범위를 넓힌다. 예컨대 "하는 법", "있습니다" 등 흔한 어미를 번갈아 사용하여 각각 최신글을 가져오고 합치는 방식이다. 이는 완벽하진 않지만 키워드 없이 최신 블로그 글을 수집하는 현실적인 방법이다.
네이버 카페 검색 API: 원리는 블로그 검색과 동일하나, 대상이 네이버 카페의 공개 글들이다. 요청 URL은 https://openapi.naver.com/v1/search/cafearticle.json이며, 파라미터와 인증 방식은 같다​
developers.naver.com
. 예를 들어 네이버 카페 최신글을 얻기 위해 "오늘"을 검색:

curl "https://openapi.naver.com/v1/search/cafearticle.json?query=%EC%98%A4%EB%8A%98&display=100&sort=date" \
     -H "X-Naver-Client-Id: <클라이언트ID>" \
     -H "X-Naver-Client-Secret: <시크릿>"


응답 구조도 비슷하며, items 배열 내 각 객체는 title, link, description, cafename(카페 이름), cafeurl 등을 포함한다​
developers.naver.com
​
developers.naver.com
. (카페글의 작성일시는 별도 필드 없이 link나 description에 포함될 수 있다.) 블로그와 마찬가지로 검색어가 결과에 미치는 영향을 줄이기 위해 범용적인 단어를 선택해야 한다.
크롤링을 통한 최신 게시물 수집: Open API는 검색어가 필수이므로 100% 완전한 “키워드 불문 최신글”을 얻기는 어려울 수 있다. 이를 보완하기 위해 웹 크롤링을 사용한다. 예를 들어 Playwright나 Selenium 같은 도구를 이용하면, 브라우저를 띄우지 않고도 검색 페이지를 열고 결과를 가져올 수 있다. 방법: Playwright Python 예시를 들면,

from playwright.sync_api import sync_playwright
with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    page = browser.new_page()
    # 네이버 통합검색에서 블로그 카테고리 + 날짜순 정렬 쿼리
    page.goto("https://search.naver.com/search.naver?where=post&query=오늘&sm=tab_opt&sort=1") 
    content = page.content()  # 결과 HTML 가져오기
    # 이후 BeautifulSoup 등으로 HTML 파싱하여 글 제목, 링크 추출
    browser.close()


위와 같이 네이버 검색 웹페이지에서 query=오늘로 검색하고 sort=1 (최신순) 옵션을 주어 결과를 가져올 수 있다. 검색어를 빈칸으로 두는 것은 허용되지 않아 임시로 "오늘"을 넣었지만, 웹 UI 상에서는 검색어를 지우고도 카테고리 최신글을 볼 수 있는 경우가 있다. (예: 네이버 블로그 홈의 최신 글 피드 등은 로그인/이웃 기반이어서 일반 수집엔 부적합) 그러므로 크롤링 접근에서도 API 방식과 동일하게 몇 가지 일반 검색어를 돌려쓰며 최신 결과를 모으는 전략을 취할 수 있다. Playwright나 Selenium은 동적 콘텐츠 로딩도 처리가 가능하므로, 필요한 경우 로그인 후 특정 카페 게시판을 훑는 작업도 구현 가능하다. 예컨대 특정 주제 카페의 새 글을 모을 때 쓸 수 있다.
수집한 블로그/카페 데이터는 뉴스와 동일한 포맷으로 에이전트에 제공한다. 각 글의 본문 전체를 얻으려면 해당 link (네이버 블로그 글 URL 등)에 접근해야 하는데, 네이버 블로그 API는 작성자 본인만이 글 내용을 가져올 수 있어 공개 API로 본문을 직접 얻기 어렵다. 대신 네이버 블로그 페이지를 스크래이핑하거나, RSS 피드 (일부 블로그는 RSS 제공) 등을 활용한다. 카페도 비슷하게 link로 접근 시 로그인 없이 볼 수 있는 게시글이라면 HTML 파싱으로 내용을 추출할 수 있다. 이러한 과정은 구현 복잡도를 고려하여, 에이전트 초기 버전에서는 제목과 요약만 가지고 LLM이 처리를 시도하게 한 뒤, 필요하면 본문 크롤링을 추가하는 단계적으로 접근한다.
4. LLM 기반 테마 추출 처리
이 절에서는 수집된 데이터로부터 급상승 테마를 식별하기 위해 LLM을 어떻게 활용하는지, 구체적인 프롬프트 설계와 입출력 형식을 제시한다. 앞서 설명한 파이프라인의 키워드 후보 추출, 의미 군집화, 테마 선정 단계를 중심으로 한다.
(1) 키워드 후보 추출 프롬프트: 에이전트는 최신 뉴스/블로그 글 텍스트를 LLM에게 주고, 이 중 빈출하거나 중요한 키워드를 뽑도록 지시한다. 예를 들어, 수집한 뉴스 기사 5건과 블로그 글 5건의 제목+요약을 하나의 프롬프트로 구성할 수 있다. 예시 입력 (일부):

뉴스1 제목: 삼성전자, 전기차 배터리 투자 확대 발표  
뉴스1 요약: 삼성전자가 전기차용 차세대 배터리 기술 개발에 5천억 투자할 계획을 발표했다...  

뉴스2 제목: LG에너지솔루션, 美 배터리 공장 증설 검토  
뉴스2 요약: LG에너지솔루션이 미국내 전기차 배터리 생산 능력 강화를 위해 공장 증설을 검토 중이라고 밝혔다...  

블로그1 제목: 테슬라 모델3 신형 시승기 – 주행감 개선  
블로그1 요약: 오늘 테슬라 모델3 개량형을 타봤습니다. 배터리 업그레이드로 주행 가능 거리가 늘었네요...  

블로그2 제목: 프로야구 개막전 직관 후기  
블로그2 요약: 드디어 2025 프로야구가 개막했습니다! 잠실구장 개막전에 다녀왔는데 경기 분위기가 최고...  

[질문] 위 글들을 바탕으로 지난 하루 동안 많이 언급된 핵심 키워드나 인물을 10개 내외로 뽑아주세요.


예시 출력: (LLM이 응답한 내용)

전기차 배터리, 삼성전자, LG에너지솔루션, 테슬라, 프로야구 개막, 잠실구장, ... 


여기서 LLM은 주어진 텍스트에서 굵직한 주제어를 추출한다. 필요 시 “쉼표로 구분된 키워드 리스트만 답하라”고 지시해 결과를 파싱하기 쉽게 만들 수 있다. 또는 JSON 배열로 달라고 해도 된다. 예를 들어 시스템 프롬프트로 {"role": "system", "content": "답변을 JSON 배열로만 출력하세요."}를 주어 키워드 리스트 형태로 응답받을 수 있다.
(2) 키워드 빈도 집계 및 전처리: 1단계 결과와 원본 데이터를 활용하여 각 키워드의 언급 횟수를 계산한다. LLM이 뽑은 키워드 외에도, 만약 특정 키워드가 뉴스/블로그에서 언급 빈도는 높으나 LLM이 간과한 경우를 대비해, 단순 빈도 상위 단어들도 참고한다. (예: TF-IDF 상위 단어 등) 그러나 시스템 복잡도를 줄이기 위해 초기에는 LLM 추출 결과만 사용해도 된다. 추출된 키워드 리스트에 대해, 수집된 문서들에서 해당 키워드가 등장한 횟수를 센다. 이때 동의어나 표현 차이를 고려해 합쳐야 하는 경우가 있으므로, 완전히 동일 문자열 매칭뿐 아니라 포함/유사 문자열 매칭도 검토한다. (예: “전기차”와 “EV”는 같은 의미로 간주) 이러한 유사어 처리는 3단계 LLM 군집화에서 다룰 것이므로, 여기서는 우선 개별 키워드 빈도만 집계한다. 그 결과 예를 들면:

전기차 배터리: 15회  
삼성전자: 9회  
LG에너지솔루션: 7회  
테슬라: 8회  
프로야구 개막: 5회  
잠실구장: 4회  
...


또한 출처별 카운트도 집계해둔다. 예: “전기차 배터리 – 뉴스 10회, 블로그 5회, 카페 0회” 등. 이 통계는 이후 결과 출력 시 활용된다.
(3) 의미 군집화 프롬프트: 2단계에서 얻은 키워드들과 빈도 데이터를 LLM에 입력하여 의미론적 테마 그룹으로 묶는다. 프롬프트에는 키워드 리스트와 각 빈도를 함께 전달하고, LLM에게 “의미가 비슷하거나 같은 사건을 가리키는 키워드들을 하나의 테마로 묶으라”는 지시를 한다. 그리고 각 테마에 대해 **이해하기 쉬운 이름(레이블)**을 만들어 달라고 한다. 예시 프롬프트:


키워드 목록과 지난 하루 언급량은 다음과 같습니다:
- 전기차 배터리 (15)
- 삼성전자 (9)
- LG에너지솔루션 (7)
- 테슬라 (8)
- 프로야구 개막 (5)
- 잠실구장 (4)
- 육아휴직 (3)
- ... (기타 키워드)

위 키워드들를 의미별로 묶어서 테마를 만들고자 합니다. 각 테마에 대해:
1. 테마 이름 (짧은 문구)
2. 해당 테마에 속한 키워드들
3. 해당 테마의 총 언급량 (괄호 안 숫자의 합)

출력은 JSON 형식의 배열로 해주세요. 예시:
[
  {"theme": "테마이름", "keywords": ["키워드1", "키워드2", ...], "mentions": 합계숫자},
  ...
]


예시 출력 (LLM 응답):

[
  {
    "theme": "전기차 배터리 기술",
    "keywords": ["전기차 배터리", "삼성전자", "LG에너지솔루션", "테슬라"],
    "mentions": 39
  },
  {
    "theme": "프로야구 2025 개막",
    "keywords": ["프로야구 개막", "잠실구장"],
    "mentions": 9
  },
  {
    "theme": "육아휴직 제도 변화",
    "keywords": ["육아휴직"],
    "mentions": 3
  },
  ...
]


위 결과에서 첫 번째 테마는 전기차 배터리 관련 키워드들을 묶어 만들었으며, mentions=39는 해당 키워드들의 합산 언급량이다. 두 번째 테마는 프로야구 개막 관련 키워드들을 묶었다. 세 번째 테마처럼 하나의 키워드만 있어도 하나의 주제가 될 수 있다. LLM은 단어 의미를 이해하여 클러스터링하므로, 사전 작업 없이도 적절히 그룹을 만들어줄 수 있다. (예: "삼성전자"와 "LG에너지솔루션"이 모두 전기차 배터리 맥락에 등장했다면 함께 묶을 수 있음) 만약 LLM이 출력한 JSON에서 잘못 묶인 항목이 있으면, 에이전트 로직에서 후처리하거나 프롬프트를 개선하여 재요청한다. 예를 들어 엉뚱하게 관련없는 키워드가 한 테마로 묶였으면, 해당 항목을 분리하도록 지시할 수 있다.
(4) z-스코어 계산 및 Top 20 선정: 3단계 출력은 지난 24시간의 테마와 그 언급량이다. 이것만으로는 **“급상승”**인지 알 수 없으므로, 과거 데이터와 비교해봐야 한다. 과거 데이터는 예를 들어 **전일 동일 시간대(4/19 15:00 기준 24시간)**의 언급량을 같은 방식으로 구해놓는다. (또는 1주일 전 등 비교 기준은 전략에 따라 정함) 각 테마별로 현재값과 과거 평균/표준편차를 넣어 z-스코어를 구한다​
stackoverflow.com
. 이 계산은 코드 상에서 수행하며, LLM에게 시키지 않는다 (LLM은 수학 계산에 취약하고 일관성 보장이 어려움​
blog.hyperarc.com
). 예를 들어 “전기차 배터리 기술” 테마의 과거 일일 언급 평균이 20, 표준편차가 5라고 하면 현재 39는 z = (39-20)/5 = 3.8이 된다. 이런 식으로 모든 테마의 z-스코어를 계산하고, 가장 높은 20개를 선택한다. 만약 테마 개수가 20 미만이라면 전부 선택한다. 선정된 테마들은 현재 시점에서 평소보다 언급량이 급증한 주제들이다. 참고: z-스코어 외에도 증가율(% 증가), 절대 언급 증가량 등을 사용할 수 있지만, z-스코어가 표준화된 척도로 편리하다. StackOverflow의 논의에 따르면 z-스코어를 쓰면 과거 평균뿐 아니라 변동성까지 고려하므로 단순 평균 대비 이상치를 더 잘 포착할 수 있다​
stackoverflow.com
.
(5) 테마별 요약 생성 프롬프트: 마지막으로 선정된 Top 20 테마 각각에 대해 LLM이 내용을 요약하도록 한다. 이때 LLM 입력에는 테마 이름과 관련 키워드, 그리고 해당 테마의 주요 출처/빈도 정보를 넣는다. LLM에게는 “왜 이 테마가 화제가 되었는지 한두 문장으로 설명하라”는 지시를 내린다. 예시 프롬프트 (테마 하나당):

테마: 전기차 배터리 기술
관련 키워드: 전기차 배터리, 삼성전자, LG에너지솔루션, 테슬라
언급량: 총 39회 (뉴스 25회, 블로그 10회, 카페 4회)

위 테마가 어떤 이슈인지 간단히 요약해 주세요.


예시 출력 (LLM 응답):

여러 완성차 및 배터리 기업들이 전기차용 차세대 배터리 기술 개발과 투자를 발표하면서 관련 논의가 크게 증가했습니다.


이러한 요약을 테마별로 생성하여 저장해둔다. 추가로, LLM을 활용해 테마별 대표 기사나 글을 지정하게 할 수도 있다 (예: “이 테마와 관련하여 가장 핵심적인 뉴스 제목을 골라라”). 그러나 우선은 단순 요약만 생성한다.
(6) 결과 병합: 최종적으로 Top 20 테마에 대한 정보 (테마명, 키워드 목록, 언급량, 출처별 비중, 요약 문장 등)을 하나의 구조로 모은다. 이는 다음 절의 출력 예시처럼 콘솔에 표시하거나, JSON 파일로 저장한다. 또한, 이 결과를 활용해 후속 자동화 (예: 블로그 포스팅 생성)에 입력으로 활용할 수 있다. 만약 자동으로 블로그에 올린다면, 각 테마 요약을 조합해 “오늘의 급상승 이슈 20선”과 같은 포맷의 글을 LLM으로 작성하게 한 뒤 API를 통해 게시할 수 있을 것이다. (네이버 블로그 Open API를 사용하면 애플리케이션 인증을 거쳐 포스팅이 가능하다.)
요약하면, LLM은 텍스트 데이터로부터 의미 추출과 해석 역할을 담당하고, 정량적인 빈도 계산이나 통계 분석은 외부 파이프라인에서 처리하여 LLM에 참고자료로 제공한다​
blog.hyperarc.com
. 이렇게 함으로써 LLM의 강점인 자연어 이해를 최대한 활용하고 약점인 수리 계산은 보완한다.
5. 결과 정리 및 출력
최종 산출물은 콘솔 형태로 Top 20 급상승 테마 목록을 표시한다. 또한 동일한 데이터를 JSON 등의 구조로도 출력하면 이후 재사용에 용이하다. 콘솔 출력은 사람이 한눈에 볼 수 있도록 랭킹 형식이나 표 형식으로 정리한다. 예를 들어 아래와 같이 순위, 테마명, 언급량 (뉴스/블로그/카페), 요약을 나열할 수 있다: 예시 콘솔 출력:

[급상승 테마 Top 20 - 2025/04/20 15:00 기준]

1. 전기차 배터리 기술 (언급량: 39회 | 뉴스 25, 블로그 10, 카페 4)  
   - 여러 완성차 및 배터리 기업의 차세대 전기차 배터리 기술 개발 발표로 관련 논의가 급증했습니다.

2. 프로야구 2025 개막 (언급량: 18회 | 뉴스 5, 블로그 8, 카페 5)  
   - 2025시즌 프로야구가 개막하면서 개막전 경기 결과와 선수단 소식이 팬 커뮤니티를 달구고 있습니다.

3. 육아휴직 제도 변화 (언급량: 12회 | 뉴스 7, 블로그 3, 카페 2)  
   - 정부의 육아휴직 기간 연장 검토 소식에 대한 관심이 높아지며 직장인 커뮤니티에서 논의가 활발합니다.

... (4위~20위 생략)


위 예시는 1~3위만 나타냈지만 실제로 20위까지 동일 형식으로 출력된다. 테마명은 LLM이 지정한 것을 사용하되 필요하면 수동 조정하고, 언급량은 총합과 함께 괄호 안에 출처별 세부치를 병기했다. 요약 문장은 LLM이 생성한 내용을 그대로 담았다. 이렇게 함으로써 사용자는 각 테마가 어떤 이슈이며 어디에서 화제가 되었는지 빠르게 파악할 수 있다. 또한 이 결과를 JSON 파일 (trending_themes.json 등)로도 저장한다고 가정하면, 형태는 다음과 같다:

[
  {
    "rank": 1,
    "theme": "전기차 배터리 기술",
    "mentions_total": 39,
    "mentions_detail": {"news": 25, "blog": 10, "cafe": 4},
    "keywords": ["전기차 배터리", "삼성전자", "LG에너지솔루션", "테슬라"],
    "summary": "여러 완성차 및 배터리 기업의 차세대 전기차 배터리 기술 개발 발표로 관련 논의가 급증했습니다."
  },
  {
    "rank": 2,
    "theme": "프로야구 2025 개막",
    "mentions_total": 18,
    "mentions_detail": {"news": 5, "blog": 8, "cafe": 5},
    "keywords": ["프로야구 개막", "잠실구장"],
    "summary": "2025시즌 프로야구가 개막하면서 개막전 경기 결과와 선수단 소식이 팬 커뮤니티를 달구고 있습니다."
  },
  ...
]


이 JSON 구조는 추후 자동 블로그 포스팅이나 리포트 생성에 입력으로 활용될 수 있다. 예컨대 각 테마를 마크다운으로 포맷팅하여 하나의 블로그 글로 작성하거나, 슬랙/카카오톡 등으로 알림을 보낼 수도 있다. 확장성: 본 설계는 모듈별로 분리되어 있어, 추후 SNS 데이터 수집 모듈(예: 트위터 트렌드나 유튜브 인기동영상 제목 등)이나 자동 게시 모듈을 추가하기 좋다. LLM 자체도 교체 가능하여, 향후 구글 Gemini API나 OpenAI의 최신 GPT 모델로 쉽게 전환할 수 있다. 실제 운영 시에는 주기적으로 (예: 매시간) 에이전트를 실행해 최신 데이터를 누적 분석함으로써, 시간 경과에 따른 트렌드 변화도 모니터링 가능할 것이다. 요약하면, 제안한 시스템은 LLM의 강력한 언어 이해능력과 전통적인 데이터 처리를 결합하여, 콘솔 환경에서 동작하는 언급량 급증 테마 추출 에이전트를 구현한다. 이는 수집→분석→요약의 전 과정을 자동화하며, 구조화된 출력으로 후속 작업까지 연결될 수 있는 유연한 아키텍처이다. 각 단계의 구체 구현 예시와 API 활용법을 본 설계서에 포함하여, 개발 시 참고할 수 있도록 하였다.​